\chapter{Identification of enriched regions using peak calling}

\section{Building a signal profile.}

The computational method of the protein-chromatin binding event identification plays a central role in ChIP-seq analysis. 
After read mapping to the reference, the next step is to identify loci with high read density comparatively to the background, referred to ChIP-seq signals, or simply peaks.
More than 30 algorithms and tools have been implemented to solve that computational problem~\cite{chen2012systematic}.
The choice of the right peak caller is crucial and depends on the type of experiment~\cite{nakato2017recent}, e.g. TF binding event identification vs. long-range histone marks interactions.

Suitable software builds a peak profile along each chromosome. 
All the peak profiles can be divided into three categories~\cite{park2009chip}. 
Sharp peaks are typical for TF due to dependency on motif sequence. 
Histones have non-specific positioning on the DNA. 
Thus the peak profile is broad and can reach several kilobases. 
The third peak type is a mix of sharp and broad signals, a typical pattern for RNA polymerase II and transcription elongation factor~\cite{lin2011dynamic}.
% \todo{to je zavádějící. Nukleosomy sice nevážou konkrétní motiv DNA, ale vykazují jistou míru sekvenčních preferencí, která se liší v závislosti na organismu.}

The very first extended set methodology to calculate peak profile density was presented in 2007~\cite{robertson2007genome}. 
Fragments are sequenced from 5' to 3' end, and the minimal enough length of a sequenced read is 36 bp long. 
But the real fragment of DNA is longer, and thus the interaction of the protein of interest is somewhere on that size selected long DNA fragment. 
Each read is computationally extended in the 3' direction. 
Regions are scored by the number of overlap reads and assessed as a candidate peak.

Sequence directionality sets the stage for a smoothed profile. 
The strand-specific read distribution form bimodal pattern combined by shifting or extending tags toward the center~\cite{valouev2008genome}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statistical model utilization for the assesment of the significance of estimated signals.}


Standard biological research has some rate of false positives. 
Due to there is no absolute proof or absolute rejection of the results in ChIP-seq studies, the analysis works with probabilities.
The end goal of the ChIP-seq experiment is the genomic loci of possible protein binding events. 
The end goal of the ChIP-seq experiment is to define the genomic loci of possible protein binding events. 
The candidate signal of a binding event is represented as a hypothesis $H_{1}$, and the null hypothesis $H_{0}$ is that there is no actual binding. To describe the statistical significance of the individual hypothesis, a P-value is calculated. 

The early approach was that the background noise is uniform; 
however, the usage of the control dataset shows that the different biases make uniform model is too ideal to be true~\cite{robertson2007genome}. 
That is why all peak calling algorithms make all the output signals associated with P-value~\cite{chitpin2019recap} to determine statistical significance in a hypothesis test. 
The null hypothesis's incorrect rejection produces the Type I error known as a "false positive". 
In the contest of ChIP-seq analysis, this type of error occurs when there is no actual binding event, but the peak caller shows that there is. 
The inversion of Type I error is Type II error, which is referred to as a "false negative". Considering the ChIP-seq experiment, the "false negative" error type is seen less serious than "false positives". 

To test peaks for significance, different peak calling algorithms adopt different statistical techniques. 
The widely used Poisson model was utilized in early software tools such as SICER~\cite{zang2009clustering}. 
The Poisson is directly connected to Binomial distribution:

\begin{align*}
    p_{k,n} = \binom{n}{k}p^k(1-p)^{n-k}
\end{align*}

where probability \textit{p} of succes in each trail and p$_{k,n}$  for \textit{k} succes in \textit{n} trails; but assosiated with rare events:

\begin{align*}
    p_{0,n} = (1 - p)^{n} = \left(1-{\frac{\lambda}{n}}\right)^{n} \to e^{-\lambda}
\end{align*}

\begin{align*}
    p_{1,n} = np(1 - p)^{n-1} = \frac{\lambda}{1-p}\left(1-{\frac{\lambda}{n}}\right)^{n} \to \lambda e^{-\lambda}
\end{align*}

\begin{align*}
    p_{2,n} = \frac{1}{2} n(n - 1) p^{2} (1-p)^{n-2} = \frac{1}{2} \frac{\lambda^{2} - \lambda p}{ (1-p)^{2}} \left(1-{\frac{\lambda}{n}}\right)^{n} \to \frac{1}{2} \lambda^{2} e^{-\lambda}
\end{align*}

For \textit{k} succes in \textit{n} trails with probability p=${\lambda / n}$ , the binomial probability p$_{n,k}$ approaches the Poisson probability:

\begin{align*}
    P_k = \frac{\lambda _i ^{k}}{k!} e^{- \lambda _i}
\end{align*}

The Poisson parameter $\lambda_i$ is supposed to be constant across the genome and provided to be inadequate for ChIP-seq peak calling. 
And the negative binomial model was suggested by CisGenome~\cite{ji2008inte}.

\begin{align*}
    NB_{y_i, \mu _i, \alpha} = \frac{\Gamma (y_i + \alpha ^{-1})}{\Gamma(\alpha ^{-1})\Gamma(y_i + 1)} \left(\frac{1}{1 + \alpha \mu_i}\right) ^{\alpha ^{-1}} \left(\frac{\alpha \mu _i}{1 + \alpha \mu _i}\right) ^{y _i}
\end{align*}

where 

\begin{align*}
    \mu _i = t _i \mu
\end{align*}

\begin{align*}
    \alpha = \frac{1}{\nu}
\end{align*}

Another suggestion was to estimate $\lambda$ for each genomic position by the local Poisson model. 
Such an approach was introduced by the most popular peak caller called MACS~\cite{zhang2008model}.
The tool slides with a constant size window across the genome, merge overlapping peaks by extending the read. 
The highest tag pileup is defined as a summit of a signal.  

\section{Multiple hypothesis testing.}

After scanning through the genome and find a large number of candidate regions, the quality of the detected peaks should be detect.
Suppose we have \textit{n} genomic loci obtained.
The $i^{th}$ null hypothesis $H_{0,i}$ with corresponding P-values $p_i$.
The global null hypothesis of simultenious test of all null hypitheses  is defined:

\begin{align*}
    H_0 = \displaystyle\bigcap_{i=1}^{n} H_{0, i}
\end{align*}

Fisher's combined probability test combines known P-values using:

\begin{align*}
    T = - \displaystyle \sum_{i=1}^{n} 2 \log p_i \sim  \chi_{2n}^{2}
\end{align*}

Bonferroni's test looks at the smallest P-value and at given desired level $\alpha$ tests the global null hypothesis by testing each $H_{0,i}$ at level $\alpha /n$. 
And whenever $p_i \leq \alpha / n$ rejects the global null hypothesis.
Assuming the global hypothesis is true, the overal level control is: 

\begin{align*}
    P_{H_0}(Error Type I) = P_{H_0} \left[\bigcup_{i=1}^{n} \left\{ p_i \leq \alpha / n\right\}\right] \leq \sum_{i=1}^{n} P_{H_0}(p_i \le \alpha / n)  = n \frac{\alpha}{n} = \alpha
\end{align*}

Even though both tests are easy and straightforward, non of them is effective. 
Fisher's test will be eliminated in a large number of the true null hypothesis. 
In the case of  Bonferroni's test, the only one P-value is used. 
Hence it can be applied only if very few binding events are expected to be significant. 
The rejection of extreme value of binding event at the $\alpha$ significance level leads to increase number of false positives. 
The idea of the global null hypothesis rejection is not suitable for genomic analysis. 
Another approach is the familywise error (FWE) rate method, which controls the probability of performing one or more Type I errors. 
However, the method is very conservative and does only a few rejections. 

The application of the False Discovery Rate (FDR) approach in the genomic field is associated with microarray technology~\cite{lai2017statistical}.
The method allows a few small rejections if the majority of the rejections are correct. Such testing adjusts the statistical confidence based on the number of tests. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statistical sin}